[
    {
        "full_name": "unslothai/unsloth",
        "stars": 45297,
        "svn_url": "https://github.com/unslothai/unsloth",
        "description": "Fine-tuning & Reinforcement Learning for LLMs. \ud83e\udda5 Train OpenAI gpt-oss, Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.",
        "topics": [
            "fine-tuning",
            "llama",
            "llms",
            "lora",
            "mistral",
            "gemma",
            "llama3",
            "unsloth",
            "llm",
            "deepseek",
            "deepseek-r1",
            "gemma3",
            "text-to-speech",
            "tts",
            "qwen",
            "qwen3",
            "agent",
            "ai",
            "openai",
            "gpt-oss"
        ],
        "owner": "https://github.com/unslothai"
    },
    {
        "full_name": "apachecn/ailearning",
        "stars": 41401,
        "svn_url": "https://github.com/apachecn/ailearning",
        "description": "AiLearning\uff1a\u6570\u636e\u5206\u6790+\u673a\u5668\u5b66\u4e60\u5b9e\u6218+\u7ebf\u6027\u4ee3\u6570+PyTorch+NLTK+TF2",
        "topics": [
            "fp-growth",
            "apriori",
            "mahchine-leaning",
            "naivebayes",
            "svm",
            "adaboost",
            "kmeans",
            "svd",
            "pca",
            "logistic",
            "regression",
            "recommendedsystem",
            "sklearn",
            "scikit-learn",
            "nlp",
            "deeplearning",
            "python",
            "dnn",
            "lstm",
            "rnn"
        ],
        "owner": "https://github.com/apachecn"
    },
    {
        "full_name": "donnemartin/data-science-ipython-notebooks",
        "stars": 28515,
        "svn_url": "https://github.com/donnemartin/data-science-ipython-notebooks",
        "description": "Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines.",
        "topics": [
            "python",
            "machine-learning",
            "deep-learning",
            "data-science",
            "big-data",
            "aws",
            "tensorflow",
            "theano",
            "caffe",
            "scikit-learn",
            "kaggle",
            "spark",
            "mapreduce",
            "hadoop",
            "matplotlib",
            "pandas",
            "numpy",
            "scipy",
            "keras"
        ],
        "owner": "https://github.com/donnemartin"
    },
    {
        "full_name": "albumentations-team/albumentations",
        "stars": 15136,
        "svn_url": "https://github.com/albumentations-team/albumentations",
        "description": "Fast and flexible image augmentation library. Paper about the library: https://www.mdpi.com/2078-2489/11/2/125",
        "topics": [
            "image-augmentation",
            "machine-learning",
            "augmentation",
            "deep-learning",
            "detection",
            "fast-augmentations",
            "segmentation",
            "image-segmentation",
            "image-processing",
            "image-classification",
            "python",
            "object-detection"
        ],
        "owner": "https://github.com/albumentations-team"
    },
    {
        "full_name": "dmlc/dgl",
        "stars": 14061,
        "svn_url": "https://github.com/dmlc/dgl",
        "description": "Python package built to ease deep learning on graph, on top of existing DL frameworks.",
        "topics": [
            "deep-learning",
            "graph-neural-networks"
        ],
        "owner": "https://github.com/dmlc"
    },
    {
        "full_name": "ydataai/ydata-profiling",
        "stars": 13130,
        "svn_url": "https://github.com/ydataai/ydata-profiling",
        "description": "1 Line of code data quality profiling & exploratory data analysis for Pandas and Spark DataFrames. ",
        "topics": [
            "pandas-profiling",
            "pandas-dataframe",
            "statistics",
            "jupyter-notebook",
            "exploration",
            "data-science",
            "python",
            "pandas",
            "machine-learning",
            "deep-learning",
            "exploratory-data-analysis",
            "eda",
            "data-quality",
            "html-report",
            "data-exploration",
            "data-analysis",
            "jupyter",
            "big-data-analytics",
            "data-profiling",
            "hacktoberfest"
        ],
        "owner": "https://github.com/ydataai"
    },
    {
        "full_name": "zalandoresearch/fashion-mnist",
        "stars": 12419,
        "svn_url": "https://github.com/zalandoresearch/fashion-mnist",
        "description": "A MNIST-like fashion product database. Benchmark :point_down: ",
        "topics": [
            "mnist",
            "deep-learning",
            "benchmark",
            "machine-learning",
            "dataset",
            "computer-vision",
            "fashion",
            "fashion-mnist",
            "gan",
            "zalando",
            "convolutional-neural-networks"
        ],
        "owner": "https://github.com/zalandoresearch"
    },
    {
        "full_name": "h2oai/h2ogpt",
        "stars": 11909,
        "svn_url": "https://github.com/h2oai/h2ogpt",
        "description": "Private chat with local GPT with document, images, video, etc. 100% private, Apache 2.0. Supports oLLaMa, Mixtral, llama.cpp, and more. Demo: https://gpt.h2o.ai/ https://gpt-docs.h2o.ai/",
        "topics": [
            "chatgpt",
            "llm",
            "ai",
            "embeddings",
            "generative",
            "gpt",
            "gpt4all",
            "pdf",
            "private",
            "privategpt",
            "vectorstore",
            "llama2",
            "mixtral"
        ],
        "owner": "https://github.com/h2oai"
    },
    {
        "full_name": "milesial/Pytorch-UNet",
        "stars": 10506,
        "svn_url": "https://github.com/milesial/Pytorch-UNet",
        "description": "PyTorch implementation of the U-Net for image semantic segmentation with high quality images",
        "topics": [
            "pytorch",
            "unet",
            "pytorch-unet",
            "semantic-segmentation",
            "convolutional-networks",
            "kaggle",
            "tensorboard",
            "wandb",
            "weights-and-biases",
            "deep-learning",
            "convolutional-neural-networks"
        ],
        "owner": "https://github.com/milesial"
    },
    {
        "full_name": "Yorko/mlcourse.ai",
        "stars": 10206,
        "svn_url": "https://github.com/Yorko/mlcourse.ai",
        "description": "Open Machine Learning Course",
        "topics": [
            "machine-learning",
            "data-analysis",
            "data-science",
            "pandas",
            "algorithms",
            "numpy",
            "scipy",
            "matplotlib",
            "seaborn",
            "plotly",
            "scikit-learn",
            "kaggle-inclass",
            "vowpal-wabbit",
            "python",
            "ipynb",
            "docker",
            "math"
        ],
        "owner": "https://github.com/Yorko"
    },
    {
        "full_name": "huggingface/transformers",
        "stars": 149379,
        "svn_url": "https://github.com/huggingface/transformers",
        "description": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
        "topics": [
            "nlp",
            "natural-language-processing",
            "pytorch",
            "pytorch-transformers",
            "transformer",
            "model-hub",
            "pretrained-models",
            "speech-recognition",
            "hacktoberfest",
            "python",
            "machine-learning",
            "deep-learning",
            "audio",
            "deepseek",
            "gemma",
            "glm",
            "llm",
            "qwen",
            "vlm"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "binary-husky/gpt_academic",
        "stars": 69213,
        "svn_url": "https://github.com/binary-husky/gpt_academic",
        "description": "\u4e3aGPT/GLM\u7b49LLM\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5b9e\u7528\u5316\u4ea4\u4e92\u63a5\u53e3\uff0c\u7279\u522b\u4f18\u5316\u8bba\u6587\u9605\u8bfb/\u6da6\u8272/\u5199\u4f5c\u4f53\u9a8c\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u5feb\u6377\u6309\u94ae&\u51fd\u6570\u63d2\u4ef6\uff0c\u652f\u6301Python\u548cC++\u7b49\u9879\u76ee\u5256\u6790&\u81ea\u8bd1\u89e3\u529f\u80fd\uff0cPDF/LaTex\u8bba\u6587\u7ffb\u8bd1&\u603b\u7ed3\u529f\u80fd\uff0c\u652f\u6301\u5e76\u884c\u95ee\u8be2\u591a\u79cdLLM\u6a21\u578b\uff0c\u652f\u6301chatglm3\u7b49\u672c\u5730\u6a21\u578b\u3002\u63a5\u5165\u901a\u4e49\u5343\u95ee, deepseekcoder, \u8baf\u98de\u661f\u706b, \u6587\u5fc3\u4e00\u8a00, llama2, rwkv, claude2, moss\u7b49\u3002",
        "topics": [
            "academic",
            "chatglm-6b",
            "chatgpt",
            "large-language-models",
            "gpt-4"
        ],
        "owner": "https://github.com/binary-husky"
    },
    {
        "full_name": "FoundationAgents/MetaGPT",
        "stars": 58332,
        "svn_url": "https://github.com/FoundationAgents/MetaGPT",
        "description": "\ud83c\udf1f The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming",
        "topics": [
            "agent",
            "gpt",
            "llm",
            "metagpt",
            "multi-agent"
        ],
        "owner": "https://github.com/FoundationAgents"
    },
    {
        "full_name": "hiyouga/LLaMA-Factory",
        "stars": 57788,
        "svn_url": "https://github.com/hiyouga/LLaMA-Factory",
        "description": "Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)",
        "topics": [
            "fine-tuning",
            "llama",
            "llm",
            "peft",
            "transformers",
            "rlhf",
            "qlora",
            "quantization",
            "qwen",
            "instruction-tuning",
            "gpt",
            "lora",
            "large-language-models",
            "agent",
            "ai",
            "moe",
            "llama3",
            "deepseek",
            "gemma",
            "nlp"
        ],
        "owner": "https://github.com/hiyouga"
    },
    {
        "full_name": "vllm-project/vllm",
        "stars": 57598,
        "svn_url": "https://github.com/vllm-project/vllm",
        "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
        "topics": [
            "gpt",
            "llm",
            "pytorch",
            "llmops",
            "mlops",
            "model-serving",
            "transformer",
            "llm-serving",
            "inference",
            "llama",
            "amd",
            "rocm",
            "cuda",
            "inferentia",
            "trainium",
            "tpu",
            "xpu",
            "hpu",
            "deepseek",
            "qwen"
        ],
        "owner": "https://github.com/vllm-project"
    },
    {
        "full_name": "unslothai/unsloth",
        "stars": 45297,
        "svn_url": "https://github.com/unslothai/unsloth",
        "description": "Fine-tuning & Reinforcement Learning for LLMs. \ud83e\udda5 Train OpenAI gpt-oss, Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.",
        "topics": [
            "fine-tuning",
            "llama",
            "llms",
            "lora",
            "mistral",
            "gemma",
            "llama3",
            "unsloth",
            "llm",
            "deepseek",
            "deepseek-r1",
            "gemma3",
            "text-to-speech",
            "tts",
            "qwen",
            "qwen3",
            "agent",
            "ai",
            "openai",
            "gpt-oss"
        ],
        "owner": "https://github.com/unslothai"
    },
    {
        "full_name": "run-llama/llama_index",
        "stars": 44154,
        "svn_url": "https://github.com/run-llama/llama_index",
        "description": "LlamaIndex is the leading framework for building LLM-powered agents over your data.",
        "topics": [
            "agents",
            "application",
            "data",
            "fine-tuning",
            "framework",
            "llamaindex",
            "llm",
            "rag",
            "vector-database",
            "multi-agents"
        ],
        "owner": "https://github.com/run-llama"
    },
    {
        "full_name": "hpcaitech/ColossalAI",
        "stars": 41147,
        "svn_url": "https://github.com/hpcaitech/ColossalAI",
        "description": "Making large AI models cheaper, faster and more accessible",
        "topics": [
            "deep-learning",
            "hpc",
            "large-scale",
            "data-parallelism",
            "pipeline-parallelism",
            "model-parallelism",
            "ai",
            "big-model",
            "distributed-computing",
            "inference",
            "heterogeneous-training",
            "foundation-models"
        ],
        "owner": "https://github.com/hpcaitech"
    },
    {
        "full_name": "harry0703/MoneyPrinterTurbo",
        "stars": 39800,
        "svn_url": "https://github.com/harry0703/MoneyPrinterTurbo",
        "description": "\u5229\u7528AI\u5927\u6a21\u578b\uff0c\u4e00\u952e\u751f\u6210\u9ad8\u6e05\u77ed\u89c6\u9891 Generate short videos with one click using AI LLM.",
        "topics": [
            "shortvideo",
            "automation",
            "chatgpt",
            "moviepy",
            "python",
            "tiktok",
            "ai"
        ],
        "owner": "https://github.com/harry0703"
    },
    {
        "full_name": "google-research/bert",
        "stars": 39503,
        "svn_url": "https://github.com/google-research/bert",
        "description": "TensorFlow code and pre-trained models for BERT",
        "topics": [
            "nlp",
            "google",
            "natural-language-processing",
            "natural-language-understanding",
            "tensorflow"
        ],
        "owner": "https://github.com/google-research"
    },
    {
        "full_name": "huggingface/transformers",
        "stars": 149379,
        "svn_url": "https://github.com/huggingface/transformers",
        "description": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
        "topics": [
            "nlp",
            "natural-language-processing",
            "pytorch",
            "pytorch-transformers",
            "transformer",
            "model-hub",
            "pretrained-models",
            "speech-recognition",
            "hacktoberfest",
            "python",
            "machine-learning",
            "deep-learning",
            "audio",
            "deepseek",
            "gemma",
            "glm",
            "llm",
            "qwen",
            "vlm"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "labmlai/annotated_deep_learning_paper_implementations",
        "stars": 63114,
        "svn_url": "https://github.com/labmlai/annotated_deep_learning_paper_implementations",
        "description": "\ud83e\uddd1\u200d\ud83c\udfeb 60+ Implementations/tutorials of deep learning papers with side-by-side notes \ud83d\udcdd; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), \ud83c\udfae reinforcement learning (ppo, dqn), capsnet, distillation, ... \ud83e\udde0",
        "topics": [
            "deep-learning",
            "deep-learning-tutorial",
            "pytorch",
            "gan",
            "transformers",
            "reinforcement-learning",
            "optimizers",
            "neural-networks",
            "transformer",
            "machine-learning",
            "attention",
            "literate-programming",
            "lora"
        ],
        "owner": "https://github.com/labmlai"
    },
    {
        "full_name": "huggingface/pytorch-image-models",
        "stars": 35223,
        "svn_url": "https://github.com/huggingface/pytorch-image-models",
        "description": "The largest collection of PyTorch image encoders / backbones. Including train, eval, inference, export scripts, and pretrained weights -- ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNetV4, MobileNet-V3 & V2, RegNet, DPN, CSPNet, Swin Transformer, MaxViT, CoAtNet, ConvNeXt, and more",
        "topics": [
            "pytorch",
            "resnet",
            "pretrained-models",
            "pretrained-weights",
            "distributed-training",
            "mobile-deep-learning",
            "mobilenet-v2",
            "mobilenetv3",
            "efficientnet",
            "augmix",
            "randaugment",
            "mixnet",
            "vision-transformer-models",
            "nfnets",
            "normalization-free-training",
            "maxvit",
            "convnext",
            "image-classification",
            "imagenet",
            "optimizer"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "lucidrains/vit-pytorch",
        "stars": 23896,
        "svn_url": "https://github.com/lucidrains/vit-pytorch",
        "description": "Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch",
        "topics": [
            "artificial-intelligence",
            "attention-mechanism",
            "transformers",
            "computer-vision",
            "image-classification"
        ],
        "owner": "https://github.com/lucidrains"
    },
    {
        "full_name": "sczhou/CodeFormer",
        "stars": 17472,
        "svn_url": "https://github.com/sczhou/CodeFormer",
        "description": "[NeurIPS 2022] Towards Robust Blind Face Restoration with Codebook Lookup Transformer",
        "topics": [
            "codebook",
            "codeformer",
            "face-enhancement",
            "face-restoration",
            "pytorch",
            "super-resolution",
            "vqgan",
            "restoration"
        ],
        "owner": "https://github.com/sczhou"
    },
    {
        "full_name": "BlinkDL/RWKV-LM",
        "stars": 13956,
        "svn_url": "https://github.com/BlinkDL/RWKV-LM",
        "description": "RWKV (pronounced RwaKuv) is an RNN with great LLM performance, which can also be directly trained like a GPT transformer (parallelizable). We are at RWKV-7 \"Goose\". So it's combining the best of RNN and transformer - great performance, linear time, constant space (no kv-cache), fast training, infinite ctx_len, and free sentence embedding.",
        "topics": [
            "attention-mechanism",
            "deep-learning",
            "gpt",
            "gpt-2",
            "gpt-3",
            "language-model",
            "linear-attention",
            "lstm",
            "pytorch",
            "rnn",
            "transformer",
            "transformers",
            "rwkv",
            "chatgpt"
        ],
        "owner": "https://github.com/BlinkDL"
    },
    {
        "full_name": "NVIDIA/Megatron-LM",
        "stars": 13534,
        "svn_url": "https://github.com/NVIDIA/Megatron-LM",
        "description": "Ongoing research training transformer models at scale",
        "topics": [
            "large-language-models",
            "model-para",
            "transformers"
        ],
        "owner": "https://github.com/NVIDIA"
    },
    {
        "full_name": "jacobgil/pytorch-grad-cam",
        "stars": 12144,
        "svn_url": "https://github.com/jacobgil/pytorch-grad-cam",
        "description": "Advanced AI Explainability for computer vision.  Support for CNNs, Vision Transformers, Classification, Object detection, Segmentation, Image similarity and more.",
        "topics": [
            "deep-learning",
            "pytorch",
            "grad-cam",
            "visualizations",
            "interpretability",
            "interpretable-ai",
            "interpretable-deep-learning",
            "score-cam",
            "class-activation-maps",
            "vision-transformers",
            "explainable-ai",
            "xai",
            "image-classification",
            "machine-learning",
            "object-detection",
            "computer-vision",
            "explainable-ml"
        ],
        "owner": "https://github.com/jacobgil"
    },
    {
        "full_name": "qubvel-org/segmentation_models.pytorch",
        "stars": 10867,
        "svn_url": "https://github.com/qubvel-org/segmentation_models.pytorch",
        "description": "Semantic segmentation models with 500+ pretrained convolutional and transformer-based backbones.",
        "topics": [
            "segmentation",
            "image-processing",
            "pspnet",
            "unet",
            "unet-pytorch",
            "pytorch",
            "fpn",
            "models",
            "imagenet",
            "semantic-segmentation",
            "image-segmentation",
            "segmentation-models",
            "unetplusplus",
            "deeplabv3",
            "deeplab-v3-plus",
            "pretrained-weights",
            "computer-vision",
            "segformer",
            "transformers",
            "dpt"
        ],
        "owner": "https://github.com/qubvel-org"
    },
    {
        "full_name": "jadore801120/attention-is-all-you-need-pytorch",
        "stars": 9371,
        "svn_url": "https://github.com/jadore801120/attention-is-all-you-need-pytorch",
        "description": "A PyTorch implementation of the Transformer model in \"Attention is All You Need\".",
        "topics": [
            "attention",
            "deep-learning",
            "attention-is-all-you-need",
            "pytorch",
            "nlp",
            "natural-language-processing"
        ],
        "owner": "https://github.com/jadore801120"
    },
    {
        "full_name": "donnemartin/data-science-ipython-notebooks",
        "stars": 28515,
        "svn_url": "https://github.com/donnemartin/data-science-ipython-notebooks",
        "description": "Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines.",
        "topics": [
            "python",
            "machine-learning",
            "deep-learning",
            "data-science",
            "big-data",
            "aws",
            "tensorflow",
            "theano",
            "caffe",
            "scikit-learn",
            "kaggle",
            "spark",
            "mapreduce",
            "hadoop",
            "matplotlib",
            "pandas",
            "numpy",
            "scipy",
            "keras"
        ],
        "owner": "https://github.com/donnemartin"
    },
    {
        "full_name": "albumentations-team/albumentations",
        "stars": 15136,
        "svn_url": "https://github.com/albumentations-team/albumentations",
        "description": "Fast and flexible image augmentation library. Paper about the library: https://www.mdpi.com/2078-2489/11/2/125",
        "topics": [
            "image-augmentation",
            "machine-learning",
            "augmentation",
            "deep-learning",
            "detection",
            "fast-augmentations",
            "segmentation",
            "image-segmentation",
            "image-processing",
            "image-classification",
            "python",
            "object-detection"
        ],
        "owner": "https://github.com/albumentations-team"
    },
    {
        "full_name": "Yorko/mlcourse.ai",
        "stars": 10206,
        "svn_url": "https://github.com/Yorko/mlcourse.ai",
        "description": "Open Machine Learning Course",
        "topics": [
            "machine-learning",
            "data-analysis",
            "data-science",
            "pandas",
            "algorithms",
            "numpy",
            "scipy",
            "matplotlib",
            "seaborn",
            "plotly",
            "scikit-learn",
            "kaggle-inclass",
            "vowpal-wabbit",
            "python",
            "ipynb",
            "docker",
            "math"
        ],
        "owner": "https://github.com/Yorko"
    },
    {
        "full_name": "autogluon/autogluon",
        "stars": 9374,
        "svn_url": "https://github.com/autogluon/autogluon",
        "description": "Fast and Accurate ML in 3 Lines of Code",
        "topics": [
            "automl",
            "machine-learning",
            "data-science",
            "deep-learning",
            "ensemble-learning",
            "computer-vision",
            "natural-language-processing",
            "structured-data",
            "object-detection",
            "gluon",
            "transfer-learning",
            "pytorch",
            "automated-machine-learning",
            "scikit-learn",
            "autogluon",
            "tabular-data",
            "hyperparameter-optimization",
            "time-series",
            "forecasting",
            "python"
        ],
        "owner": "https://github.com/autogluon"
    },
    {
        "full_name": "microsoft/RD-Agent",
        "stars": 7604,
        "svn_url": "https://github.com/microsoft/RD-Agent",
        "description": "Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. \ud83d\udd17https://aka.ms/RD-Agent-Tech-Report",
        "topics": [
            "agent",
            "ai",
            "automation",
            "data-mining",
            "data-science",
            "development",
            "llm",
            "research"
        ],
        "owner": "https://github.com/microsoft"
    },
    {
        "full_name": "aymericdamien/TopDeepLearning",
        "stars": 6065,
        "svn_url": "https://github.com/aymericdamien/TopDeepLearning",
        "description": "A list of popular github projects related to deep learning",
        "topics": [
            "deep-learning",
            "machine-learning",
            "tensorflow",
            "pytorch"
        ],
        "owner": "https://github.com/aymericdamien"
    },
    {
        "full_name": "obss/sahi",
        "stars": 4792,
        "svn_url": "https://github.com/obss/sahi",
        "description": "Framework agnostic sliced/tiled inference + interactive ui + error analysis plots",
        "topics": [
            "object-detection",
            "instance-segmentation",
            "computer-vision",
            "small-object-detection",
            "large-image",
            "mmdetection",
            "pytorch",
            "python",
            "coco",
            "deep-learning",
            "machine-learning",
            "remote-sensing",
            "huggingface",
            "fiftyone",
            "satellite",
            "tiling",
            "merge",
            "explainable-ai",
            "oriented-object-detection",
            "yolo11"
        ],
        "owner": "https://github.com/obss"
    },
    {
        "full_name": "pytorch/ignite",
        "stars": 4696,
        "svn_url": "https://github.com/pytorch/ignite",
        "description": "High-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.",
        "topics": [
            "pytorch",
            "neural-network",
            "python",
            "machine-learning",
            "deep-learning",
            "metrics",
            "hacktoberfest",
            "closember"
        ],
        "owner": "https://github.com/pytorch"
    },
    {
        "full_name": "TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials",
        "stars": 3898,
        "svn_url": "https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials",
        "description": "A comprehensive list of Deep Learning / Artificial Intelligence and Machine Learning tutorials - rapidly expanding into areas of AI/Deep Learning / Machine Vision / NLP and industry specific areas such as Climate / Energy, Automotives, Retail, Pharma, Medicine, Healthcare, Policy, Ethics and more.",
        "topics": [
            "machine-learning",
            "deep-learning",
            "tensorflow",
            "python",
            "pytorch",
            "keras",
            "lua",
            "matplotlib",
            "aws",
            "kaggle",
            "pandas",
            "scikit-learn",
            "torch",
            "artificial-intelligence",
            "neural-network",
            "convolutional-neural-networks",
            "tensorflow-tutorials",
            "python-data",
            "ipython-notebook",
            "capsule-network"
        ],
        "owner": "https://github.com/TarrySingh"
    },
    {
        "full_name": "Charmve/Surface-Defect-Detection",
        "stars": 3749,
        "svn_url": "https://github.com/Charmve/Surface-Defect-Detection",
        "description": "\ud83d\udcc8 \u76ee\u524d\u6700\u5927\u7684\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u6570\u636e\u5e93\u53ca\u8bba\u6587\u96c6 Constantly summarizing open source dataset and critical papers in the field of surface defect research which are of great importance.  ",
        "topics": [
            "surface-detection",
            "surface-defects",
            "image-segmentation",
            "pcb-surface-defect",
            "surface-defect-detection",
            "paper",
            "defects",
            "dataset",
            "surface",
            "deep-learning",
            "charmve"
        ],
        "owner": "https://github.com/Charmve"
    },
    {
        "full_name": "donnemartin/data-science-ipython-notebooks",
        "stars": 28515,
        "svn_url": "https://github.com/donnemartin/data-science-ipython-notebooks",
        "description": "Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines.",
        "topics": [
            "python",
            "machine-learning",
            "deep-learning",
            "data-science",
            "big-data",
            "aws",
            "tensorflow",
            "theano",
            "caffe",
            "scikit-learn",
            "kaggle",
            "spark",
            "mapreduce",
            "hadoop",
            "matplotlib",
            "pandas",
            "numpy",
            "scipy",
            "keras"
        ],
        "owner": "https://github.com/donnemartin"
    },
    {
        "full_name": "Charmve/Surface-Defect-Detection",
        "stars": 3749,
        "svn_url": "https://github.com/Charmve/Surface-Defect-Detection",
        "description": "\ud83d\udcc8 \u76ee\u524d\u6700\u5927\u7684\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u6570\u636e\u5e93\u53ca\u8bba\u6587\u96c6 Constantly summarizing open source dataset and critical papers in the field of surface defect research which are of great importance.  ",
        "topics": [
            "surface-detection",
            "surface-defects",
            "image-segmentation",
            "pcb-surface-defect",
            "surface-defect-detection",
            "paper",
            "defects",
            "dataset",
            "surface",
            "deep-learning",
            "charmve"
        ],
        "owner": "https://github.com/Charmve"
    },
    {
        "full_name": "benfred/implicit",
        "stars": 3707,
        "svn_url": "https://github.com/benfred/implicit",
        "description": "Fast Python Collaborative Filtering for Implicit Feedback Datasets",
        "topics": [
            "collaborative-filtering",
            "machine-learning",
            "matrix-factorization",
            "recommender-system",
            "recommendation-system",
            "recommendation"
        ],
        "owner": "https://github.com/benfred"
    },
    {
        "full_name": "huggingface/datasets",
        "stars": 20618,
        "svn_url": "https://github.com/huggingface/datasets",
        "description": "\ud83e\udd17 The largest hub of ready-to-use datasets for AI models with fast, easy-to-use and efficient data manipulation tools",
        "topics": [
            "nlp",
            "datasets",
            "pytorch",
            "tensorflow",
            "pandas",
            "numpy",
            "natural-language-processing",
            "computer-vision",
            "machine-learning",
            "deep-learning",
            "speech",
            "ai",
            "artificial-intelligence",
            "llm",
            "dataset-hub"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "open-compass/opencompass",
        "stars": 6008,
        "svn_url": "https://github.com/open-compass/opencompass",
        "description": "OpenCompass is an LLM evaluation platform, supporting a wide range of models (Llama3, Mistral, InternLM2,GPT-4,LLaMa2, Qwen,GLM, Claude, etc) over 100+ datasets.",
        "topics": [
            "evaluation",
            "benchmark",
            "large-language-model",
            "chatgpt",
            "llm",
            "llama2",
            "openai",
            "llama3"
        ],
        "owner": "https://github.com/open-compass"
    },
    {
        "full_name": "huggingface/transformers",
        "stars": 149379,
        "svn_url": "https://github.com/huggingface/transformers",
        "description": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
        "topics": [
            "nlp",
            "natural-language-processing",
            "pytorch",
            "pytorch-transformers",
            "transformer",
            "model-hub",
            "pretrained-models",
            "speech-recognition",
            "hacktoberfest",
            "python",
            "machine-learning",
            "deep-learning",
            "audio",
            "deepseek",
            "gemma",
            "glm",
            "llm",
            "qwen",
            "vlm"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "hpcaitech/ColossalAI",
        "stars": 41147,
        "svn_url": "https://github.com/hpcaitech/ColossalAI",
        "description": "Making large AI models cheaper, faster and more accessible",
        "topics": [
            "deep-learning",
            "hpc",
            "large-scale",
            "data-parallelism",
            "pipeline-parallelism",
            "model-parallelism",
            "ai",
            "big-model",
            "distributed-computing",
            "inference",
            "heterogeneous-training",
            "foundation-models"
        ],
        "owner": "https://github.com/hpcaitech"
    },
    {
        "full_name": "google-research/bert",
        "stars": 39503,
        "svn_url": "https://github.com/google-research/bert",
        "description": "TensorFlow code and pre-trained models for BERT",
        "topics": [
            "nlp",
            "google",
            "natural-language-processing",
            "natural-language-understanding",
            "tensorflow"
        ],
        "owner": "https://github.com/google-research"
    },
    {
        "full_name": "2noise/ChatTTS",
        "stars": 37780,
        "svn_url": "https://github.com/2noise/ChatTTS",
        "description": "A generative speech model for daily dialogue.",
        "topics": [
            "agent",
            "text-to-speech",
            "chat",
            "chatgpt",
            "chattts",
            "chinese",
            "chinese-language",
            "english",
            "english-language",
            "gpt",
            "llm",
            "llm-agent",
            "natural-language-inference",
            "python",
            "torch",
            "torchaudio",
            "tts"
        ],
        "owner": "https://github.com/2noise"
    },
    {
        "full_name": "huggingface/pytorch-image-models",
        "stars": 35223,
        "svn_url": "https://github.com/huggingface/pytorch-image-models",
        "description": "The largest collection of PyTorch image encoders / backbones. Including train, eval, inference, export scripts, and pretrained weights -- ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNetV4, MobileNet-V3 & V2, RegNet, DPN, CSPNet, Swin Transformer, MaxViT, CoAtNet, ConvNeXt, and more",
        "topics": [
            "pytorch",
            "resnet",
            "pretrained-models",
            "pretrained-weights",
            "distributed-training",
            "mobile-deep-learning",
            "mobilenet-v2",
            "mobilenetv3",
            "efficientnet",
            "augmix",
            "randaugment",
            "mixnet",
            "vision-transformer-models",
            "nfnets",
            "normalization-free-training",
            "maxvit",
            "convnext",
            "image-classification",
            "imagenet",
            "optimizer"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "huggingface/diffusers",
        "stars": 30685,
        "svn_url": "https://github.com/huggingface/diffusers",
        "description": "\ud83e\udd17 Diffusers: State-of-the-art diffusion models for image, video, and audio generation in PyTorch.",
        "topics": [
            "deep-learning",
            "diffusion",
            "image-generation",
            "pytorch",
            "score-based-generative-modeling",
            "image2image",
            "text2image",
            "stable-diffusion",
            "stable-diffusion-diffusers",
            "latent-diffusion-models",
            "flux",
            "image2video",
            "text2video",
            "video2video",
            "qwen-image"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "tatsu-lab/stanford_alpaca",
        "stars": 30146,
        "svn_url": "https://github.com/tatsu-lab/stanford_alpaca",
        "description": "Code and documentation to train Stanford's Alpaca models, and generate the data.",
        "topics": [
            "deep-learning",
            "instruction-following",
            "language-model"
        ],
        "owner": "https://github.com/tatsu-lab"
    },
    {
        "full_name": "huggingface/datasets",
        "stars": 20618,
        "svn_url": "https://github.com/huggingface/datasets",
        "description": "\ud83e\udd17 The largest hub of ready-to-use datasets for AI models with fast, easy-to-use and efficient data manipulation tools",
        "topics": [
            "nlp",
            "datasets",
            "pytorch",
            "tensorflow",
            "pandas",
            "numpy",
            "natural-language-processing",
            "computer-vision",
            "machine-learning",
            "deep-learning",
            "speech",
            "ai",
            "artificial-intelligence",
            "llm",
            "dataset-hub"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "QwenLM/Qwen",
        "stars": 19223,
        "svn_url": "https://github.com/QwenLM/Qwen",
        "description": "The official repo of Qwen (\u901a\u4e49\u5343\u95ee) chat & pretrained large language model proposed by Alibaba Cloud.",
        "topics": [
            "chinese",
            "large-language-models",
            "natural-language-processing",
            "flash-attention",
            "llm",
            "pretrained-models"
        ],
        "owner": "https://github.com/QwenLM"
    },
    {
        "full_name": "nari-labs/dia",
        "stars": 18289,
        "svn_url": "https://github.com/nari-labs/dia",
        "description": "A TTS model capable of generating ultra-realistic dialogue in one pass.",
        "topics": [
            "ai",
            "open-weight",
            "text-to-speech"
        ],
        "owner": "https://github.com/nari-labs"
    },
    {
        "full_name": "huggingface/transformers",
        "stars": 149379,
        "svn_url": "https://github.com/huggingface/transformers",
        "description": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
        "topics": [
            "nlp",
            "natural-language-processing",
            "pytorch",
            "pytorch-transformers",
            "transformer",
            "model-hub",
            "pretrained-models",
            "speech-recognition",
            "hacktoberfest",
            "python",
            "machine-learning",
            "deep-learning",
            "audio",
            "deepseek",
            "gemma",
            "glm",
            "llm",
            "qwen",
            "vlm"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "huggingface/pytorch-image-models",
        "stars": 35223,
        "svn_url": "https://github.com/huggingface/pytorch-image-models",
        "description": "The largest collection of PyTorch image encoders / backbones. Including train, eval, inference, export scripts, and pretrained weights -- ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNetV4, MobileNet-V3 & V2, RegNet, DPN, CSPNet, Swin Transformer, MaxViT, CoAtNet, ConvNeXt, and more",
        "topics": [
            "pytorch",
            "resnet",
            "pretrained-models",
            "pretrained-weights",
            "distributed-training",
            "mobile-deep-learning",
            "mobilenet-v2",
            "mobilenetv3",
            "efficientnet",
            "augmix",
            "randaugment",
            "mixnet",
            "vision-transformer-models",
            "nfnets",
            "normalization-free-training",
            "maxvit",
            "convnext",
            "image-classification",
            "imagenet",
            "optimizer"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "BlinkDL/RWKV-LM",
        "stars": 13956,
        "svn_url": "https://github.com/BlinkDL/RWKV-LM",
        "description": "RWKV (pronounced RwaKuv) is an RNN with great LLM performance, which can also be directly trained like a GPT transformer (parallelizable). We are at RWKV-7 \"Goose\". So it's combining the best of RNN and transformer - great performance, linear time, constant space (no kv-cache), fast training, infinite ctx_len, and free sentence embedding.",
        "topics": [
            "attention-mechanism",
            "deep-learning",
            "gpt",
            "gpt-2",
            "gpt-3",
            "language-model",
            "linear-attention",
            "lstm",
            "pytorch",
            "rnn",
            "transformer",
            "transformers",
            "rwkv",
            "chatgpt"
        ],
        "owner": "https://github.com/BlinkDL"
    },
    {
        "full_name": "jacobgil/pytorch-grad-cam",
        "stars": 12144,
        "svn_url": "https://github.com/jacobgil/pytorch-grad-cam",
        "description": "Advanced AI Explainability for computer vision.  Support for CNNs, Vision Transformers, Classification, Object detection, Segmentation, Image similarity and more.",
        "topics": [
            "deep-learning",
            "pytorch",
            "grad-cam",
            "visualizations",
            "interpretability",
            "interpretable-ai",
            "interpretable-deep-learning",
            "score-cam",
            "class-activation-maps",
            "vision-transformers",
            "explainable-ai",
            "xai",
            "image-classification",
            "machine-learning",
            "object-detection",
            "computer-vision",
            "explainable-ml"
        ],
        "owner": "https://github.com/jacobgil"
    },
    {
        "full_name": "qubvel-org/segmentation_models.pytorch",
        "stars": 10867,
        "svn_url": "https://github.com/qubvel-org/segmentation_models.pytorch",
        "description": "Semantic segmentation models with 500+ pretrained convolutional and transformer-based backbones.",
        "topics": [
            "segmentation",
            "image-processing",
            "pspnet",
            "unet",
            "unet-pytorch",
            "pytorch",
            "fpn",
            "models",
            "imagenet",
            "semantic-segmentation",
            "image-segmentation",
            "segmentation-models",
            "unetplusplus",
            "deeplabv3",
            "deeplab-v3-plus",
            "pretrained-weights",
            "computer-vision",
            "segformer",
            "transformers",
            "dpt"
        ],
        "owner": "https://github.com/qubvel-org"
    },
    {
        "full_name": "EleutherAI/gpt-neox",
        "stars": 7295,
        "svn_url": "https://github.com/EleutherAI/gpt-neox",
        "description": "An implementation of model parallel autoregressive transformers on GPUs, based on the Megatron and DeepSpeed libraries",
        "topics": [
            "deepspeed-library",
            "gpt-3",
            "transformers",
            "language-model"
        ],
        "owner": "https://github.com/EleutherAI"
    },
    {
        "full_name": "clovaai/donut",
        "stars": 6530,
        "svn_url": "https://github.com/clovaai/donut",
        "description": "Official Implementation of OCR-free Document Understanding Transformer (Donut) and Synthetic Document Generator (SynthDoG), ECCV 2022",
        "topics": [
            "document-ai",
            "eccv-2022",
            "multimodal-pre-trained-model",
            "ocr",
            "nlp",
            "computer-vision"
        ],
        "owner": "https://github.com/clovaai"
    },
    {
        "full_name": "lucidrains/DALLE-pytorch",
        "stars": 5627,
        "svn_url": "https://github.com/lucidrains/DALLE-pytorch",
        "description": "Implementation / replication of DALL-E, OpenAI's Text to Image Transformer, in Pytorch",
        "topics": [
            "artificial-intelligence",
            "deep-learning",
            "attention-mechanism",
            "text-to-image",
            "transformers",
            "multi-modal"
        ],
        "owner": "https://github.com/lucidrains"
    },
    {
        "full_name": "ThilinaRajapakse/simpletransformers",
        "stars": 4214,
        "svn_url": "https://github.com/ThilinaRajapakse/simpletransformers",
        "description": "Transformers for Information Retrieval, Text Classification, NER, QA, Language Modelling, Language Generation, T5, Multi-Modal, and Conversational AI",
        "topics": [
            "transformers",
            "text-classification",
            "named-entity-recognition",
            "question-answering",
            "conversational-ai",
            "information-retrival"
        ],
        "owner": "https://github.com/ThilinaRajapakse"
    },
    {
        "full_name": "huggingface/optimum",
        "stars": 3074,
        "svn_url": "https://github.com/huggingface/optimum",
        "description": "\ud83d\ude80 Accelerate inference and training of \ud83e\udd17 Transformers, Diffusers, TIMM and Sentence Transformers with easy to use hardware optimization tools",
        "topics": [
            "onnx",
            "pytorch",
            "inference",
            "training",
            "intel",
            "graphcore",
            "onnxruntime",
            "transformers",
            "quantization",
            "habana",
            "optimization",
            "tflite"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "BlinkDL/RWKV-LM",
        "stars": 13956,
        "svn_url": "https://github.com/BlinkDL/RWKV-LM",
        "description": "RWKV (pronounced RwaKuv) is an RNN with great LLM performance, which can also be directly trained like a GPT transformer (parallelizable). We are at RWKV-7 \"Goose\". So it's combining the best of RNN and transformer - great performance, linear time, constant space (no kv-cache), fast training, infinite ctx_len, and free sentence embedding.",
        "topics": [
            "attention-mechanism",
            "deep-learning",
            "gpt",
            "gpt-2",
            "gpt-3",
            "language-model",
            "linear-attention",
            "lstm",
            "pytorch",
            "rnn",
            "transformer",
            "transformers",
            "rwkv",
            "chatgpt"
        ],
        "owner": "https://github.com/BlinkDL"
    },
    {
        "full_name": "Morizeyao/GPT2-Chinese",
        "stars": 7585,
        "svn_url": "https://github.com/Morizeyao/GPT2-Chinese",
        "description": "Chinese version of GPT2 training code, using BERT tokenizer.",
        "topics": [
            "transformer",
            "gpt-2",
            "chinese",
            "nlp",
            "text-generation"
        ],
        "owner": "https://github.com/Morizeyao"
    },
    {
        "full_name": "binary-husky/gpt_academic",
        "stars": 69213,
        "svn_url": "https://github.com/binary-husky/gpt_academic",
        "description": "\u4e3aGPT/GLM\u7b49LLM\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5b9e\u7528\u5316\u4ea4\u4e92\u63a5\u53e3\uff0c\u7279\u522b\u4f18\u5316\u8bba\u6587\u9605\u8bfb/\u6da6\u8272/\u5199\u4f5c\u4f53\u9a8c\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u5feb\u6377\u6309\u94ae&\u51fd\u6570\u63d2\u4ef6\uff0c\u652f\u6301Python\u548cC++\u7b49\u9879\u76ee\u5256\u6790&\u81ea\u8bd1\u89e3\u529f\u80fd\uff0cPDF/LaTex\u8bba\u6587\u7ffb\u8bd1&\u603b\u7ed3\u529f\u80fd\uff0c\u652f\u6301\u5e76\u884c\u95ee\u8be2\u591a\u79cdLLM\u6a21\u578b\uff0c\u652f\u6301chatglm3\u7b49\u672c\u5730\u6a21\u578b\u3002\u63a5\u5165\u901a\u4e49\u5343\u95ee, deepseekcoder, \u8baf\u98de\u661f\u706b, \u6587\u5fc3\u4e00\u8a00, llama2, rwkv, claude2, moss\u7b49\u3002",
        "topics": [
            "academic",
            "chatglm-6b",
            "chatgpt",
            "large-language-models",
            "gpt-4"
        ],
        "owner": "https://github.com/binary-husky"
    },
    {
        "full_name": "hiyouga/LLaMA-Factory",
        "stars": 57789,
        "svn_url": "https://github.com/hiyouga/LLaMA-Factory",
        "description": "Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)",
        "topics": [
            "fine-tuning",
            "llama",
            "llm",
            "peft",
            "transformers",
            "rlhf",
            "qlora",
            "quantization",
            "qwen",
            "instruction-tuning",
            "gpt",
            "lora",
            "large-language-models",
            "agent",
            "ai",
            "moe",
            "llama3",
            "deepseek",
            "gemma",
            "nlp"
        ],
        "owner": "https://github.com/hiyouga"
    },
    {
        "full_name": "unslothai/unsloth",
        "stars": 45297,
        "svn_url": "https://github.com/unslothai/unsloth",
        "description": "Fine-tuning & Reinforcement Learning for LLMs. \ud83e\udda5 Train OpenAI gpt-oss, Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.",
        "topics": [
            "fine-tuning",
            "llama",
            "llms",
            "lora",
            "mistral",
            "gemma",
            "llama3",
            "unsloth",
            "llm",
            "deepseek",
            "deepseek-r1",
            "gemma3",
            "text-to-speech",
            "tts",
            "qwen",
            "qwen3",
            "agent",
            "ai",
            "openai",
            "gpt-oss"
        ],
        "owner": "https://github.com/unslothai"
    },
    {
        "full_name": "tatsu-lab/stanford_alpaca",
        "stars": 30146,
        "svn_url": "https://github.com/tatsu-lab/stanford_alpaca",
        "description": "Code and documentation to train Stanford's Alpaca models, and generate the data.",
        "topics": [
            "deep-learning",
            "instruction-following",
            "language-model"
        ],
        "owner": "https://github.com/tatsu-lab"
    },
    {
        "full_name": "jingyaogong/minimind",
        "stars": 25916,
        "svn_url": "https://github.com/jingyaogong/minimind",
        "description": "\ud83d\ude80\ud83d\ude80 \u300c\u5927\u6a21\u578b\u300d2\u5c0f\u65f6\u5b8c\u5168\u4ece0\u8bad\u7ec326M\u7684\u5c0f\u53c2\u6570GPT\uff01\ud83c\udf0f Train a 26M-parameter GPT from scratch in just 2h!",
        "topics": [
            "artificial-intelligence",
            "large-language-model"
        ],
        "owner": "https://github.com/jingyaogong"
    },
    {
        "full_name": "huggingface/peft",
        "stars": 19525,
        "svn_url": "https://github.com/huggingface/peft",
        "description": "\ud83e\udd17 PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.",
        "topics": [
            "adapter",
            "diffusion",
            "llm",
            "parameter-efficient-learning",
            "python",
            "pytorch",
            "transformers",
            "lora",
            "fine-tuning",
            "peft"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "QwenLM/Qwen",
        "stars": 19223,
        "svn_url": "https://github.com/QwenLM/Qwen",
        "description": "The official repo of Qwen (\u901a\u4e49\u5343\u95ee) chat & pretrained large language model proposed by Alibaba Cloud.",
        "topics": [
            "chinese",
            "large-language-models",
            "natural-language-processing",
            "flash-attention",
            "llm",
            "pretrained-models"
        ],
        "owner": "https://github.com/QwenLM"
    },
    {
        "full_name": "ymcui/Chinese-LLaMA-Alpaca",
        "stars": 18918,
        "svn_url": "https://github.com/ymcui/Chinese-LLaMA-Alpaca",
        "description": "\u4e2d\u6587LLaMA&Alpaca\u5927\u8bed\u8a00\u6a21\u578b+\u672c\u5730CPU/GPU\u8bad\u7ec3\u90e8\u7f72 (Chinese LLaMA & Alpaca LLMs)",
        "topics": [
            "llm",
            "plm",
            "pre-trained-language-models",
            "alpaca",
            "llama",
            "nlp",
            "quantization",
            "large-language-models",
            "lora",
            "alpaca-2",
            "llama-2"
        ],
        "owner": "https://github.com/ymcui"
    },
    {
        "full_name": "LlamaFamily/Llama-Chinese",
        "stars": 14688,
        "svn_url": "https://github.com/LlamaFamily/Llama-Chinese",
        "description": "Llama\u4e2d\u6587\u793e\u533a\uff0c\u5b9e\u65f6\u6c47\u603b\u6700\u65b0Llama\u5b66\u4e60\u8d44\u6599\uff0c\u6784\u5efa\u6700\u597d\u7684\u4e2d\u6587Llama\u5927\u6a21\u578b\u5f00\u6e90\u751f\u6001\uff0c\u5b8c\u5168\u5f00\u6e90\u53ef\u5546\u7528",
        "topics": [
            "llama",
            "llm",
            "pretraining",
            "agent",
            "llama4",
            "rl"
        ],
        "owner": "https://github.com/LlamaFamily"
    },
    {
        "full_name": "BlinkDL/RWKV-LM",
        "stars": 13956,
        "svn_url": "https://github.com/BlinkDL/RWKV-LM",
        "description": "RWKV (pronounced RwaKuv) is an RNN with great LLM performance, which can also be directly trained like a GPT transformer (parallelizable). We are at RWKV-7 \"Goose\". So it's combining the best of RNN and transformer - great performance, linear time, constant space (no kv-cache), fast training, infinite ctx_len, and free sentence embedding.",
        "topics": [
            "attention-mechanism",
            "deep-learning",
            "gpt",
            "gpt-2",
            "gpt-3",
            "language-model",
            "linear-attention",
            "lstm",
            "pytorch",
            "rnn",
            "transformer",
            "transformers",
            "rwkv",
            "chatgpt"
        ],
        "owner": "https://github.com/BlinkDL"
    },
    {
        "full_name": "huggingface/transformers",
        "stars": 149380,
        "svn_url": "https://github.com/huggingface/transformers",
        "description": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
        "topics": [
            "nlp",
            "natural-language-processing",
            "pytorch",
            "pytorch-transformers",
            "transformer",
            "model-hub",
            "pretrained-models",
            "speech-recognition",
            "hacktoberfest",
            "python",
            "machine-learning",
            "deep-learning",
            "audio",
            "deepseek",
            "gemma",
            "glm",
            "llm",
            "qwen",
            "vlm"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "binary-husky/gpt_academic",
        "stars": 69213,
        "svn_url": "https://github.com/binary-husky/gpt_academic",
        "description": "\u4e3aGPT/GLM\u7b49LLM\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5b9e\u7528\u5316\u4ea4\u4e92\u63a5\u53e3\uff0c\u7279\u522b\u4f18\u5316\u8bba\u6587\u9605\u8bfb/\u6da6\u8272/\u5199\u4f5c\u4f53\u9a8c\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u5feb\u6377\u6309\u94ae&\u51fd\u6570\u63d2\u4ef6\uff0c\u652f\u6301Python\u548cC++\u7b49\u9879\u76ee\u5256\u6790&\u81ea\u8bd1\u89e3\u529f\u80fd\uff0cPDF/LaTex\u8bba\u6587\u7ffb\u8bd1&\u603b\u7ed3\u529f\u80fd\uff0c\u652f\u6301\u5e76\u884c\u95ee\u8be2\u591a\u79cdLLM\u6a21\u578b\uff0c\u652f\u6301chatglm3\u7b49\u672c\u5730\u6a21\u578b\u3002\u63a5\u5165\u901a\u4e49\u5343\u95ee, deepseekcoder, \u8baf\u98de\u661f\u706b, \u6587\u5fc3\u4e00\u8a00, llama2, rwkv, claude2, moss\u7b49\u3002",
        "topics": [
            "academic",
            "chatglm-6b",
            "chatgpt",
            "large-language-models",
            "gpt-4"
        ],
        "owner": "https://github.com/binary-husky"
    },
    {
        "full_name": "hiyouga/LLaMA-Factory",
        "stars": 57789,
        "svn_url": "https://github.com/hiyouga/LLaMA-Factory",
        "description": "Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)",
        "topics": [
            "fine-tuning",
            "llama",
            "llm",
            "peft",
            "transformers",
            "rlhf",
            "qlora",
            "quantization",
            "qwen",
            "instruction-tuning",
            "gpt",
            "lora",
            "large-language-models",
            "agent",
            "ai",
            "moe",
            "llama3",
            "deepseek",
            "gemma",
            "nlp"
        ],
        "owner": "https://github.com/hiyouga"
    },
    {
        "full_name": "hpcaitech/ColossalAI",
        "stars": 41147,
        "svn_url": "https://github.com/hpcaitech/ColossalAI",
        "description": "Making large AI models cheaper, faster and more accessible",
        "topics": [
            "deep-learning",
            "hpc",
            "large-scale",
            "data-parallelism",
            "pipeline-parallelism",
            "model-parallelism",
            "ai",
            "big-model",
            "distributed-computing",
            "inference",
            "heterogeneous-training",
            "foundation-models"
        ],
        "owner": "https://github.com/hpcaitech"
    },
    {
        "full_name": "microsoft/JARVIS",
        "stars": 24343,
        "svn_url": "https://github.com/microsoft/JARVIS",
        "description": "JARVIS, a system to connect LLMs with ML community. Paper: https://arxiv.org/pdf/2303.17580.pdf",
        "topics": [
            "deep-learning",
            "platform",
            "pytorch"
        ],
        "owner": "https://github.com/microsoft"
    },
    {
        "full_name": "fishaudio/fish-speech",
        "stars": 22898,
        "svn_url": "https://github.com/fishaudio/fish-speech",
        "description": "SOTA Open Source TTS",
        "topics": [
            "llama",
            "transformer",
            "tts",
            "valle",
            "vits",
            "vqgan",
            "vqvae"
        ],
        "owner": "https://github.com/fishaudio"
    },
    {
        "full_name": "huggingface/peft",
        "stars": 19525,
        "svn_url": "https://github.com/huggingface/peft",
        "description": "\ud83e\udd17 PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.",
        "topics": [
            "adapter",
            "diffusion",
            "llm",
            "parameter-efficient-learning",
            "python",
            "pytorch",
            "transformers",
            "lora",
            "fine-tuning",
            "peft"
        ],
        "owner": "https://github.com/huggingface"
    },
    {
        "full_name": "graphdeco-inria/gaussian-splatting",
        "stars": 18408,
        "svn_url": "https://github.com/graphdeco-inria/gaussian-splatting",
        "description": "Original reference implementation of \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\"",
        "topics": [
            "computer-graphics",
            "computer-vision",
            "radiance-field"
        ],
        "owner": "https://github.com/graphdeco-inria"
    },
    {
        "full_name": "nari-labs/dia",
        "stars": 18289,
        "svn_url": "https://github.com/nari-labs/dia",
        "description": "A TTS model capable of generating ultra-realistic dialogue in one pass.",
        "topics": [
            "ai",
            "open-weight",
            "text-to-speech"
        ],
        "owner": "https://github.com/nari-labs"
    },
    {
        "full_name": "camel-ai/owl",
        "stars": 18024,
        "svn_url": "https://github.com/camel-ai/owl",
        "description": "\ud83e\udd89 OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation",
        "topics": [
            "agent",
            "artificial-intelligence",
            "multi-agent-systems",
            "task-automation",
            "web-interaction"
        ],
        "owner": "https://github.com/camel-ai"
    }
]